#!/usr/bin/env python
# encoding: utf-8
"""
spankisplice.py

Input junction coverage, AStalavista splicing definitions (Sammeth et. al)
Output compiled data by event.

To build AStalavista event definitions, see here:
http://genome.crg.es/astalavista/

"""
from __future__ import division 

import operator # For sorting dictionaries

import re
import sys
import argparse
import pysam
import collections
import math
import numpy
import csv
import os

from pyfasta import Fasta

# Custom modules to import:
import spanki.spanki_parse_utils as spanki_parse_utils
import spanki.spanki_utils as spanki_utils

from datetime import datetime, date



class Junctionid:
	"""
	Base class for a junction, from juncid
	Expects junction id
	"""
	def __init__(self, juncid):
		chr = juncid.split(':')[0]
		coords = juncid.split(':')[1]
		strand = juncid.split(':')[2]
		start = int(coords.split('_')[0]) - 1
		end = int(coords.split('_')[1])
		self.chr = chr
		self.start = start
		self.end = end
		self.strand = strand.strip()
		self.intronsize = end - start
		#self.accid = str(coords.split('_')[0])
		if self.strand == "+":
			self.donid = chr + ":" + str(start)
			self.accid = chr + ":" + str(end)
		elif self.strand == "-":
			self.accid = chr + ":" + str(start)
			self.donid = chr + ":" + str(end)
		else:
			print "strand character is", self.strand
			quit("Don't recognize strand")

def GTFtoDict(infn):
    #logging.info("Parsing the GTF file %s." %(infn))
    gtflines = []
    infile = open(infn, 'r')
    numlines = 0
    for line in infile:
        numlines = numlines + 1
        gtflines.append(parseGTFlineToDict(line))

	#print "Processed %d lines in %s."
    #logging.info("Processed %d lines in %s." %(numlines, infn))
    infile.close()

    return gtflines

def addAttributesToGTFline(linedict):
    attributes = linedict["attribute"].split(";")
    del attributes[-1]
    attributes = [x.strip() for x in attributes]
    
    for attrib in attributes:
        attr = attrib.strip().split(" ")
        linedict[attr[0]] = attr[1].strip("\"")
    return linedict

def parseGTFlineToDict(line):
    values = line.split("\t")
    keys = ["seqname", "source", "feature", "start", "end", "score",
            "strand", "unknown", "attribute"]
    linedict = dict(zip(keys, values))
    linedict = addAttributesToGTFline(linedict)
    return linedict



def crossTabulate(x):
	D = collections.defaultdict(int)
	for y in x:
		D[y] += 1
	x = D.keys()
	x.sort()
	for x in D.keys():
		print "\t", x, "\t", D[x]

def readCtab(fname):
	lines = csv.DictReader(open(fname, 'rb'), delimiter='\t')
	CTAB = {}
	for line in lines:
		CTAB[line['tracking_id']] = line
	return CTAB

def readJtab(jfile):
	"""
	Reads in junction table
	Assumes first line has fields
	"""
	JTAB = {}
	lines = csv.reader(open(jfile, 'rb'), delimiter='\t')
	linecount = 0
	for line in lines:
		if (linecount < 1):
			#keys = line.split(" ")
			keys = line
			#print keys
		else: 
			values = line
			linedict = dict(zip(keys, values))
			juncid = str(linedict['juncid']).rstrip(" ")
			JTAB[juncid] = linedict 
		linecount += 1
	return JTAB


def chain_to_joins(flankstring,chainstring,chr,strand,eventcode):
	
	chains = chainstring.split(",");  # List of sites
	flanks = flankstring.split(",");  # The flanks (Always 2?)
	# Skipped exon example:
	#flanks "110877^,111907-"; structure "0,1-2^"; splice_chain ",111005-111117^"

	if eventcode == "AltFE":
		temp = chains[0]
		pattern = re.compile('\d+\^')
		m = pattern.search(chains[0])
		try:
			donor1 = m.group(0)
		except:
			donor1 = "none"
		m = pattern.search(chains[1])
		try:
			donor2 = m.group(0)
		except:
			donor2 = "none"
		path1 = str(donor1) + str(flanks[1])
		path2 = str(donor2) + str(flanks[1])
	elif eventcode == "AltLE":
		pattern = re.compile('\d+\-')
		m = pattern.search(chains[0])
		try:
			acceptor1 = m.group(0)
		except:
			acceptor1 = "none"
		m = pattern.search(chains[1])
		try:
			acceptor2 = m.group(0)
		except:
			acceptor2 = "none"
		path1 = flanks[0] + acceptor1
		path2 = flanks[0] + acceptor2
	else:
		path1 = flanks[0] + chains[0] + flanks[1];
		path2 = flanks[0] + chains[1] + flanks[1];
	
	# Traverse through each path.  Make a join for all cases where there is [0-9]-[0-9]^
	# Add correction to standardized id

	p = re.compile('\d+\^\d+\-')
	joins1 = []
	coords = p.findall(path1)
	for coord in coords:
		q = re.compile('\d+')
		x = q.findall(coord)
		if x[0] < x[1]: joins1.append(chr + ":" + str(int(x[0]) + 1) + "_" + str(int(x[1]) - 1) + ":" + strand)
		else: joins1.append(chr + ":" + str(int(x[1]) + 1) + "_" + str(int(x[0]) - 1) + ":" + strand)
	joins2 = []
	coords = p.findall(path2)
	#print "----- coords ----"
	#print coords
	#print "-----------------"
	for coord in coords:
		q = re.compile('\d+')
		x = q.findall(coord)
		if x[0] < x[1]: joins2.append(chr + ":" + str(int(x[0]) + 1) + "_" + str(int(x[1]) - 1) + ":" + strand)
		else: joins2.append(chr + ":" + str(int(x[1]) + 1) + "_" + str(int(x[0]) - 1) + ":" + strand)
		
	if len(joins1) < 1: 
		if flanks[0] == "null":
			joins1.append("none")
		else:
			joins1.append("irt")
	if len(joins2) < 1: joins2.append("none")
	joinstring = ",".join(joins1) + ";" + ",".join(joins2) 

	#Retained intron
	#chr2L	Undefined	as_event	155333	155784	1.39999	+	.	transcript_id "FBtr0078118,FBtr0301887"; gene_id "chr2L:155333-157666W"; flanks "155333(,155784^"; structure "0,1^2-"; splice_chain ",155429^155546-"; sources "Undefined,Undefined"; dimension "2_5"; degree "4"; localization "5UTR-CDS_maxCDS"; 
	#chr2L	Undefined	as_event	155333	155784	1.39999	+	.	transcript_id "FBtr0078118,FBtr0301886"; gene_id "chr2L:155333-157666W"; flanks "155333(,155784^"; structure "0,1^2-"; splice_chain ",155410^155466-"; sources "Undefined,Undefined"; dimension "2_5"; degree "4"; localization "5UTR-CDS_maxCDS"; 
	#chrX	Undefined	as_event	20344594	20345509	2.00000	-	.	transcript_id "FBtr0077320,FBtr0301537"; gene_id "chrX:20342674-20362133C"; flanks "20345509-,20344594^"; structure "0,1^2-"; splice_chain ",20345249^20345035-"; sources "Undefined,Undefined"; dimension "2_2"; degree "4"; localization "CDS_maxCDS"; 
	
	#Note:  None of them have "null" in the flanks field
	
	#Exon skip
	#chr2L	Undefined	as_event	110877	111907	1.13333	+	.	transcript_id "FBtr00                 "; gene_id "chr2L:106903-114433W"; flanks "110877^,111907-"; structure "0,1-2^"; splice_chain ",111005-111117^"; sources "Undefined,Undefined"; dimension "2_2"; degree "4"; localization "CDS_maxCDS"; 
	
	#No junction to assay
	#chrX	Undefined	as_event	21484626	21486039	2.00000	-	.	transcript_id "FBtr0070053,FBtr0070054"; gene_id "chrX:21482818-21486039C"; flanks "null,21484626^"; structure "1(2^4-,3("; splice_chain "21486039(21485651^21484978-,21485062("; sources "Undefined,Undefined"; dimension "2_2"; degree "4"; localization "5UTR-CDS_maxCDS"; 
	
	return joinstring;


def geneidsFromTxlist(txlist,lookup):
	geneids = {}
	genenames = {}
	for tx in txlist:
		#print tx
		#print lookup[tx]
		try:
			geneids[lookup[tx]["gene_id"]] = 1
		except:
			pass
		try: 
			genenames[lookup[tx]["gene_name"]] = 1
		except:
			pass
	gidlist = geneids.keys()
	gids = str(','.join(gidlist))
	gnamelist = genenames.keys()
	gnames = str(','.join(gnamelist))
	return gids,gnames


def summarize_events(eventdict,outfile):

	namedevents = collections.defaultdict(lambda : collections.defaultdict(dict))	
	unclassifiedevents = collections.defaultdict(lambda : collections.defaultdict(dict))	
	
	for event in eventdict.keys():
		"""
		Iterate over each item in eventdict
		"""	
		astacode = eventdict[event]['eventcode']
		if astacode in namedevents.keys():
			namedevents[astacode] += 1
		else:
			namedevents[astacode] = 1
		if astacode == "Unclassified":
			if eventdict[event]['structure'] in unclassifiedevents.keys():
				unclassifiedevents[eventdict[event]['structure']] += 1
			else:
				unclassifiedevents[eventdict[event]['structure']] = 1

	print  >> outfile, "Here is the breakdown for named events:"
	for code in namedevents.keys():
		print >> outfile, "\t", code, "-->", namedevents[code]
	print  >> outfile, "These are unclassified structures that have >= 10 events"
	sorted_x = sorted(unclassifiedevents.iteritems(), key=operator.itemgetter(1))
	totalnoname = 0
	for y in sorted_x:
		if (y[1] >= 10):
			print  >> outfile, "\t**", y[0], '\t', y[1]
			#quit()
		totalnoname += y[1]
	print >> outfile, "Total EVENTS with no english translation:  ", totalnoname
	print >> outfile, "Total STRUCTURES with no english translation:  ", len(unclassifiedevents.keys())

def countEventTypes(ECODES, astalines):

	Ks = ECODES.keys()
	
	EVENTCOUNT = {}
	NONAMEEVENTS = {}
	
	for line in astalines:
		"""
		Iterate over each line in asta def file
		"""	
		#print line['gene_id']
		astacode = line['structure']
		if astacode in Ks:
			eventcode = ECODES[astacode]
			#print astacode, "is", eventcode
			if EVENTCOUNT.has_key(eventcode): 
				EVENTCOUNT[eventcode] += 1
			else:
				EVENTCOUNT[eventcode] = 1
		else:
			if NONAMEEVENTS.has_key(astacode): 
				NONAMEEVENTS[astacode] += 1
			else:
				NONAMEEVENTS[astacode] = 1
	return EVENTCOUNT, NONAMEEVENTS

def print_events(NAMED,UNNAMED):
	print >> sys.stderr, "[%s] Counts of event types:" % (spanki_utils.timestamp())
	print >> sys.stderr, "[%s] Frequent occuring non-categorized:" % (spanki_utils.timestamp())
	sorted_x = sorted(UNNAMED.iteritems(), key=operator.itemgetter(1))
	totalnoname = 0
	totalevents = 0
	for y in sorted_x:
		if (y[1] >= 100):
			print y[0], '\t', y[1]
		totalnoname += y[1]
	print "Total non-characterized:  ", totalnoname
	for key in NAMED.keys():
		print key, '=>', NAMED[key]
		totalevents += NAMED[key]
 	totalevents += totalnoname
 	print "There are ", totalevents, " total events"

	# A sample entry
	#{'attribute': 'transcript_id "FBtr0089158,FBtr0089162"; gene_id "chr4:89956-129430W"; flanks "null,92947-"; structure "1(2^,3(4^"; splice_chain "89956(90020^,91032(91193^";
	#sources "Undefined,Undefined"; dimension "2_6"; degree "4"; localization "5UTR_maxCDS"; \n', 'joins2': ['chr4:91194_92946:+'], 'joins1': ['chr4:90021_92946:+'], 
	#'sources': 'Undefined,Undefined', 'splice_chain': '89956(90020^,91032(91193^', 'seqname': 'chr4', 'end': '92947', 'eventcode': 'altFE', 'source': 'Undefined', 'unknown': '.',
	#'feature': 'as_event', 'start': '89956', 'score': '1.22222', 'strand': '+', 'gnames': 'FBgn0085432', 'degree': '4', 'localization': '5UTR_maxCDS', 'gene_id': 'chr4:89956-129430W', 
	#'transcript_id': 'FBtr0089158,FBtr0089162', 'flanks': 'null,92947-', 'structure': '1(2^,3(4^', 'gids': 'pan', 'txlist2': ['FBtr0089162'], 'txlist1': ['FBtr0089158', 'FBtr0089162'], 
	#'joinstring': 'chr4:90021_92946:+;chr4:91194_92946:+', 'dimension': '2_6'}

		
def astacode_to_english(astacode):
	eventcode = ""
	fepattern = re.compile('\(')
	lepattern = re.compile('\)')
	ECODES = {}
	ECODES["1-2),3-4)"] = "AltLE"
	ECODES["1(2^,3(4^"] = "AltFE"
	ECODES["1-2^,3-4^"] = "mutexcl"
	ECODES["0,1-2^"] = "exonskip"
	ECODES["1^,2^"] = "altdonor"
	ECODES["1-,2-"] = "altacceptor"
	ECODES["0,1^2-"] = "retintron"
	ECODES["0,1-2^3-4^"] = "skip2exons"
	ECODES["1^4-,2^3-"] = "altdon_altacc"
	ECODES["1^3-,2^4-"] = "altdon_altacc"
	
	if astacode in ECODES.keys():
		eventcode = ECODES[astacode]
	else:
		fe = fepattern.search(astacode)
		le = lepattern.search(astacode)
		if (fe > 0): eventcode = "AltFE"
		elif (le > 0): eventcode = "AltLE"
		else: 
			eventcode = "Unclassified"
		if (fe > 0) & (le > 0): 
			eventcode = "FELE"
	
	return eventcode


def parse_asta_defs(line,lookup):
	"""
	Classify event types
	Identify inclusion/exclusion paths
	Generate "joinstrings"
	Add geneid using lookup table
	"""
	linedict = {}

	flankstring = line['flanks']
	chainstring = line['splice_chain']
	chr = line['seqname']
	astacode = line['structure']
	strand = line['strand']
	transcripts = line['transcript_id']
	astagene = line['gene_id']
	"""
	Get geneid from txids
	"""
	txie = transcripts.split(',')
	txlist1 = txie[0].split('/')
	txlist2 = txie[1].split('/')
	txlist = txlist1
	txlist.extend(txlist2)
	gids, gnames = geneidsFromTxlist(txlist,lookup)
	
	eventcode = astacode_to_english(astacode)
			
	joinstring = chain_to_joins(flankstring,chainstring,chr,strand,eventcode)

	"""
	Excluding some events
	(Be conservative)
	"""
	
	if (astacode == "0,0") or (eventcode == "FELE"):
		pass
	else:
		'''
		Instantiate events
		'''
		linedict = line
		linedict['joinstring'] = joinstring
		joins = joinstring.split(";")
		joinstring1 = joins[0]
		joinstring2 = joins[1]
		joins1 = joinstring1.split(',')
		joins2 = joinstring2.split(',')
		'''
		Adjustment for alt LE
		Only use most 5' join
		'''
		if eventcode == "AltLE":
			linedict['joins1'] = [joins1[0]]
			linedict['joins2'] = [joins2[0]]		
		else:
			linedict['joins1'] = joins1
			linedict['joins2'] = joins2
		linedict['txlist1'] = txlist1
		linedict['txlist2'] = txlist2
		linedict['eventcode'] = eventcode
		linedict['gene_name'] = gnames
		linedict['gene_id'] = gids
	return linedict


def get_coverage(eventdict,DATAB):
	# Cov each path
	# joins each path
	# sites each path
	# inc (cov / sites) 
	# exc (cov / sites)
	
	#DSX joinid
	#testid = "chr3R:3761376_3761489:-"
	psi_errors = 0
	adj_ok = 0
	for eventid in eventdict.keys():	
		cov1 = 0
		cov2 = 0
		#joins1 = eventdict[eventid]['joins1']
		#joins2 = eventdict[eventid]['joins2']
		joins = eventdict[eventid]['joinstring'].split(";")
		joins1 = joins[0].split(",")
		joins2 = joins[1].split(",")

		join1COV = []
		join2COV = []
		sites1 = len(joins1)
		sites2 = len(joins2)

		for join in joins1:
			if join == 'irt':
				'''
				If a retained intron, get irt in other join(s)
				Remember that there may be more than one join in the otherpath!
				Make number of sites equal to number of joins in other path times 2 
				'''
				sites1 = sites2 * 2
				try:
					tempcov = 0
					for x in joins2:
						tempcov += int(JTAB[x]['irt'])
					join1COV.append(tempcov)
				except KeyError:
					join1COV.append(0)
			else:
				try:
					join1COV.append(int(JTAB[join]['cov']))
				except KeyError:
					join1COV.append(0)

		for join in joins2:
			try:
				join2COV.append(int(JTAB[join]['cov']))
			except KeyError:
				join2COV.append(0)
	
		#inc = sum(join1COV)
		#exc = sum(join2COV)
		cov1 = sum(join1COV)
		cov2 = sum(join2COV)
		inc = cov1 / sites1
		exc = cov2 / sites2
		'''
		Check that number of sites = number of coverage values
		'''
		if (sites1 != len(join1COV)) and (sites2 != len(join2COV)):
			print "Warning: number of sites doesn't match coverage string"
		############################################################
		############################################################
		############################################################
		'''
		# Find out if there is coverage to an edge outside the event
		'''
		
		confound_cov1 = 0
		confound_cov2 = 0

		'''
		Find junction coverage to edges outside of the 
		event, in either path
		'''
		for juncid in joins1[:-1]:
			j1 = Junctionid(juncid)
			covtotal = 0
			for acceptor in DATAB[j1.donid].keys():
				if DATAB[j1.donid][acceptor]:
					covtotal += int(DATAB[j1.donid][acceptor])
			if covtotal > DATAB[j1.donid][j1.accid]:
				'''
				Path 1 has confounding coverage
				'''
				confound_cov1 = confound_cov1 + (covtotal - DATAB[j1.donid][j1.accid])
		for juncid in joins2[:-1]:
			j1 = Junctionid(juncid)
			covtotal = 0
			for acceptor in DATAB[j1.donid].keys():
				if DATAB[j1.donid][acceptor]:
					covtotal += int(DATAB[j1.donid][acceptor])
			if covtotal > DATAB[j1.donid][j1.accid]:
				'''
				Path 2 has confounding coverage
				'''
				confound_cov2 = confound_cov2 + (covtotal - DATAB[j1.donid][j1.accid])
				
		############################################################
		############################################################
		############################################################

			
		'''
		Find the 'precise' inclusion/exclusion
		'''
		#inc_adj = join1COV[0]
		#exc_adj = join2COV[0]
		
		cov1_adj = join1COV[0]
		cov2_adj = join2COV[0]
		sites1_adj = 1
		sites2_adj = 1
		inc_adj = cov1_adj
		exc_adj = cov2_adj
		
		'''
		Do this part differently if retained intron
		'''

		if (eventdict[eventid]['eventcode'] == "retintron"):
			cov1_adj = cov1
			cov2_adj = cov2
			sites1_adj = sites1
			sites2_adj = sites2
			inc_adj = cov1
			exc_adj = cov2
		

		'''
		Calculate psi (for both forms)
		'''
		try:
			#psi = (inc / sites1) / ((inc / sites1) + (exc / sites2))
			psi = (inc) / ((inc) + (exc))
		except ZeroDivisionError:
			psi = 0
		try:
			psi_adj = (inc_adj) / ((inc_adj) + (exc_adj))
		except ZeroDivisionError:
			psi_adj = 0

		joinstring_adj = joins1[0] + ";" + joins2[0]

		###############
		# Add results to eventdict
		###############

        #'joinstring','inc_sites','exc_sites','inc','exc', 'psi',		
		#eventdict[eventid]['inc'] = inc
		#eventdict[eventid]['exc'] = exc
		#eventdict[eventid]['inc_sites'] = sites1 
		#eventdict[eventid]['exc_sites'] = sites2 
		#eventdict[eventid]['psi'] = "%.3f" % psi
		#eventdict[eventid]['psi_adj'] = "%.3f" % psi_adj		

		## Sensitive mode
		eventdict[eventid]['joinstring_sens'] = eventdict[eventid]['joinstring']
		eventdict[eventid]['inccov_sens'] = cov1
		eventdict[eventid]['exccov_sens'] = cov2
		eventdict[eventid]['totalcov_sens'] = cov1 + cov2
		eventdict[eventid]['avgcovpersite_sens'] = (inc + exc) / 2
		eventdict[eventid]['inc_sites_sens'] = sites1 
		eventdict[eventid]['exc_sites_sens'] = sites2 
		eventdict[eventid]['inc_sens'] = inc
		eventdict[eventid]['exc_sens'] = exc
		eventdict[eventid]['psi_sens'] = "%.3f" % psi

		## Confounding coverage
		eventdict[eventid]['confounding_inccov'] = confound_cov1
		eventdict[eventid]['confounding_exccov'] = confound_cov2

		## Precise mode
		eventdict[eventid]['joinstring_prec'] = joinstring_adj
		eventdict[eventid]['inccov_prec'] = cov1_adj
		eventdict[eventid]['exccov_prec'] = cov2_adj
		eventdict[eventid]['totalcov_prec'] = cov1_adj + cov2_adj
		eventdict[eventid]['avgcovpersite_prec'] = (inc_adj + exc_adj) / 2
		eventdict[eventid]['inc_sites_prec'] = sites1_adj
		eventdict[eventid]['exc_sites_prec'] = sites2_adj
		eventdict[eventid]['inc_prec'] = inc_adj
		eventdict[eventid]['exc_prec'] = exc_adj
		eventdict[eventid]['psi_prec'] = "%.3f" % psi_adj


		## This is where choice of sens or prec is made.
		## Should be deprecated, since 'opt' is no longer reported. 
		
		if (confound_cov1 > 0) | (confound_cov2 > 0):
			eventdict[eventid]['opt_calc_type'] = "prec"
			#eventdict[eventid]['joinstring'] =  eventdict[eventid]['joinstring_prec']
			#eventdict[eventid]['inccov'] = cov1_adj
			#eventdict[eventid]['exccov'] = cov2_adj
			#eventdict[eventid]['totalcov'] = cov1_adj + cov2_adj
			#eventdict[eventid]['avgcovpersite'] = (inc_adj + exc_adj) / 2
			#eventdict[eventid]['inc_sites'] = sites1_adj
			#eventdict[eventid]['exc_sites'] = sites2_adj
			#eventdict[eventid]['inc'] = inc_adj
			#eventdict[eventid]['exc'] = exc_adj
			#eventdict[eventid]['psi'] = "%.3f" % psi_adj
		else:
			eventdict[eventid]['opt_calc_type'] = "sens"
			#eventdict[eventid]['joinstring'] =  eventdict[eventid]['joinstring_sens']
			#eventdict[eventid]['inccov'] = cov1
			#eventdict[eventid]['exccov'] = cov2
			#eventdict[eventid]['totalcov'] = cov1 + cov2
			#eventdict[eventid]['avgcovpersite'] = (inc + exc) / 2
			#eventdict[eventid]['inc_sites'] = sites1 
			#eventdict[eventid]['exc_sites'] = sites2 
			#eventdict[eventid]['inc'] = inc
			#eventdict[eventid]['exc'] = exc
			#eventdict[eventid]['psi'] = "%.3f" % psi

		
		## This is redundant:
		#eventdict[eventid]['joinstring'] =  eventdict[eventid]['joinstring']
		#eventdict[eventid]['inccov'] = eventdict[eventid]['inccov']
		#eventdict[eventid]['exccov'] = eventdict[eventid]['exccov'] 
		#eventdict[eventid]['totalcov'] = eventdict[eventid]['totalcov']
		#eventdict[eventid]['avgcovpersite'] = eventdict[eventid]['avgcovpersite']
		#eventdict[eventid]['inc_sites'] = eventdict[eventid]['inc_sites']
		#eventdict[eventid]['exc_sites'] = eventdict[eventid]['exc_sites']
		#eventdict[eventid]['inc'] = eventdict[eventid]['inc']
		#eventdict[eventid]['exc'] = eventdict[eventid]['exc']
		#eventdict[eventid]['psi'] = eventdict[eventid]['psi']



		'''
		Adding transcripts to event results dict
		'''	
		tx1 = eventdict[eventid]['txlist1']
		tx2 = eventdict[eventid]['txlist2']
		
		'''
		Incorporating FPKMs
		'''	
		fpkm1 = 0
		fpkm2 = 0		
		for tx in tx1:
			try: 
				fpkm1 += float(CTAB[tx]['FPKM'])
			except KeyError:
				fpkm1 += 0
		for tx in tx2:
			try:
				fpkm2 += float(CTAB[tx]['FPKM'])
			except KeyError:
				fpkm2 += 0
		eventdict[eventid]['fpkm1'] = "%.3f" % fpkm1
		eventdict[eventid]['fpkm2'] = "%.3f" % fpkm2
		try:
			eventdict[eventid]['fpkm_proportion'] = "%.3f" % (fpkm1 / (fpkm1 + fpkm2))
		except:
			eventdict[eventid]['fpkm_proportion'] = "Low data"

	
def print_output_table(mydict,handle,fieldorder):
	print >> handle, "id\t", '\t'.join(map(str,fieldorder))
	for myid in sorted(mydict.keys()):
		vals = []
		for field in fieldorder:
			vals.append(mydict[myid][field])
		print >> handle, myid, "\t", '\t'.join(map(str,vals))

def print_output_table_firstiskey(mydict,handle,fieldorder):
	print >> handle, '\t'.join(map(str,fieldorder))
	for myid in sorted(mydict.keys()):
		vals = []
		for field in fieldorder[1:]:
			vals.append(mydict[myid][field])
		print >> handle, myid, "\t", '\t'.join(map(str,vals))


def uniq(seq): 
	# Not order preserving
	keys = {} 
	for e in seq:
		keys[e] = 1 
	return keys.keys()


class MyParser(argparse.ArgumentParser):
    def error(self, message):
        sys.stderr.write('error: %s\n' % message)
        #sys.stderr.write(desc)
        self.print_help()
        sys.exit(2)

def parseions(desc):
	#parser=MyParser(description=desc, formatter_class=argparse.ArgumentDefaultsHelpFormatter)
	parser=MyParser(description=desc, formatter_class=argparse.RawTextHelpFormatter)
	parser.add_argument('-g', help='Reference GTF', action="store", dest="g")
	parser.add_argument('-a', action="store", dest="a",
		help="AStalavista definitions\n"
		"Generated from the reference GTF you are using")
	parser.add_argument('-jtab', action="store", dest="jtab", 
		help="junctiontab (jtab)\n"
		"From spankijunc or merge_jtabs")
	parser.add_argument('-f', action="store", dest="f",
		help="Fasta file\n"
		"Must have same chromosomes as GTF")
	parser.add_argument('-c', action="store", dest="c",
		help="Cufflinks output\n"
		"The isoforms.fpkm_tracking file, to extract FPKMs (optional)")
	parser.add_argument('-o', help="Output directory, default='spankisplice_out'", action="store", dest="o", default="spankisplice_out")
	parser.add_argument('-v', help="Vebosely report event tables, default=F", action="store", dest="v", default="F")
	args = parser.parse_args()
	if len(sys.argv)==1:
		parser.print_help()
		sys.exit(1)
	return args

# Initialize parameters
desc = '''
-----------------------------------------------------------------
spankisplice - Quantify splicing events in a sample

Input junction coverage, AStalavista splicing definitions (Sammeth et al.)
Output compiled data by event.

To build AStalavista event definitions, see here:
http://genome.crg.es/astalavista/
-----------------------------------------------------------------
'''

args = parseions(desc)
outfile = args.o
gtffile = args.g
jfile = args.jtab
cfile = args.c
astafile = args.a
fastafile = args.f
verbose = args.v



# Checking that input files exist
if not os.path.exists(gtffile):
	print >> sys.stderr, "\n******************************************"
	print >> sys.stderr, "** Error: Could not find input gtf file **"
	print >> sys.stderr, "******************************************\n\n"	
	parser=MyParser(description=desc, formatter_class=argparse.RawTextHelpFormatter)
	parser.print_help()
	exit(1)
if not os.path.exists(fastafile):
	print >> sys.stderr, "\n******************************************"
	print >> sys.stderr, "** Error: Could not find input fasta file **"
	print >> sys.stderr, "******************************************\n\n"	
	parser=MyParser(description=desc, formatter_class=argparse.RawTextHelpFormatter)
	parser.print_help()
	exit(1)
if not os.path.exists(astafile):
	print >> sys.stderr, "\n******************************************"
	print >> sys.stderr, "** Error: Could not find input asta tab **"
	print >> sys.stderr, "******************************************\n\n"	
	parser=MyParser(description=desc, formatter_class=argparse.RawTextHelpFormatter)
	parser.print_help()
	exit(1)
if not os.path.exists(jfile):
	print >> sys.stderr, "\n******************************************"
	print >> sys.stderr, "** Error: Could not find input jtab    **"
	print >> sys.stderr, "******************************************\n\n"	
	parser=MyParser(description=desc, formatter_class=argparse.RawTextHelpFormatter)
	parser.print_help()
	exit(1)



# Prepare output directory
output_dir = outfile
spanki_utils.prepare_output_dir(output_dir)
# Prepare output file names

asta_counts_out_sens_name = output_dir + "/events.out"
asta_counts_out_sens = open(asta_counts_out_sens_name, "w")

if (verbose != "F"):
	# Report 'precise' table only if in verbose mode
	
	# These two tables are deprecated
	#asta_counts_out_opt_name = output_dir + "/events_opt.out"
	#asta_counts_out_opt = open(asta_counts_out_opt_name, "w")
	
	#asta_counts_out_all_name = output_dir + "/events_all.out"
	#asta_counts_out_all = open(asta_counts_out_all_name, "w")
	
	asta_counts_out_prec_name = output_dir + "/events_prec.out"
	asta_counts_out_prec = open(asta_counts_out_prec_name, "w")


unassayable_events_counts_out_name = output_dir + "/unassayable_events.out"
unassayable_events_out = open(unassayable_events_counts_out_name, "w")

asta_summary_out_name = output_dir + "/event_summary.out"
asta_summary_out = open(asta_summary_out_name, "w")
    
def main():
	
	########################################
	# Setting up and checking
	########################################
 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 	# Checking that required files are present
 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	if not fastafile:
		quit("Need to specify a fasta file")
	if not gtffile:
		quit("Need to specify a gtf file")
		 
	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 	# Intializing the reference
 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 	# You need the gtf file, and the fasta file
 	lookup = spanki_utils.prep_ref(gtffile,fastafile,output_dir)
  	## Note that you now have a reference called ref.bam, and a lookup dict
	tmp_dir = output_dir + "/tmp/"
 	reffile = tmp_dir + "/ref.bam"

 	########################################
	########################################
	# Loading in data
	########################################
	########################################
 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 	# Loading jtab , which has coverage and IRT
 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	# Load jtab
	print >> sys.stderr, "[%s] Loading junction data %s" % (spanki_utils.timestamp(), jfile)
	"""
	This version expects there to be a header, and parses all fields
	"""
	global JTAB
	JTAB = readJtab(jfile)
	myjuncs =  JTAB.keys()
	
	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 	# Loading FPKMs from cufflinks
 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	print >> sys.stderr, "[%s] Loading transcript FPKMs %s" % (spanki_utils.timestamp(), cfile)
	"""
	Expects a  isoforms.fpkm_tracking
	"""
	global CTAB
	CTAB = {}

	
	if cfile:
		CTAB = readCtab(cfile)
	
	#quit()
	
 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 	# Loading asta defs
 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 	
	print >> sys.stderr, "[%s] Loading asta defs %s" % (spanki_utils.timestamp(), astafile)
	astalines = GTFtoDict(astafile)

	# data look like this:	
	#{'seqname': 'chr2L', 'end': '9484', 'localization': 'CDS-3UTR_maxCDS', 'degree': '4', 'start': '8193', 'unknown': '.', 'flanks': '8193-,9484)', 'feature': 'as_event', 'gene_id': 'chr2L:7529-9484W', 'source': 'Undefined', 'score': '2.00000', 'sources': 'Undefined,Undefined', 'structure': '0,1^2-', 'splice_chain': ',8589^8668-', 'attribute': 'transcript_id "FBtr0300689,FBtr0300690"; gene_id "chr2L:7529-9484W"; flanks "8193-,9484)"; structure "0,1^2-"; splice_chain ",8589^8668-"; sources "Undefined,Undefined"; dimension "2_2"; degree "4"; localization "CDS-3UTR_maxCDS"; \n', 'transcript_id': 'FBtr0300689,FBtr0300690', 'dimension': '2_2', 'strand': '+'}
	

	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	# Making a covbyedge hash from input jtab
	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	print >> sys.stderr, "[%s] Making a donor/acceptor neighbor coverage hash" % (spanki_utils.timestamp())
	#print >> sys.stderr, "[%s] There are"  % (spanki_utils.timestamp()), len(keys1), "juncids in the OKlist" % (spanki_utils.timestamp())
	covbyedge = collections.defaultdict(lambda : collections.defaultdict(dict))
	DATAB = collections.defaultdict(lambda : collections.defaultdict(dict))
	
	#counter = 0
	for juncid in myjuncs:
		j1 = Junctionid(juncid)
		'''
		Only instantiate if coverage > 0
		'''
		if (int(JTAB[juncid]['cov']) > 0):	
			if covbyedge[j1.donid]:
				covbyedge[j1.donid] += int(JTAB[juncid]['cov'])
			else:
				covbyedge[j1.donid] = int(JTAB[juncid]['cov'])
			if covbyedge[j1.accid]:
				covbyedge[j1.accid] += int(JTAB[juncid]['cov'])
			else:
				covbyedge[j1.accid] = int(JTAB[juncid]['cov'])
			if DATAB[j1.donid][j1.accid]:
				DATAB[j1.donid][j1.accid] += int(JTAB[juncid]['cov'])
			else:
				DATAB[j1.donid][j1.accid] = int(JTAB[juncid]['cov'])


 	########################################
	########################################
	# Analyses
	########################################
	########################################

 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 	# Build intron chains
 	# Get coordinates to measure
 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 	
	print >> sys.stderr, "[%s] Iterating through events" % (spanki_utils.timestamp())
	
	# Try iterating over lines, calling sub many times
	eventdict = collections.defaultdict(lambda : collections.defaultdict(dict))	
	unassayable_events = collections.defaultdict(lambda : collections.defaultdict(dict))	
	counter = 0
	excluded = 0
	for line in astalines:
		linedict = parse_asta_defs(line,lookup)
		if linedict:
			counter += 1		
			eventid = 'ASTA' + "%07d" % (counter,)
			#print eventid, line
			joins = []
			joins.extend(linedict['joins2'])
			joins.extend(linedict['joins1'])
			#print joins
			if 'none' in joins:
				unassayable_events[eventid] = linedict
			else:
				eventdict[eventid] = linedict
		else:
			excluded += 1
	print "Processed", counter + excluded, "events"
	print "Excluded", excluded, "events"

	print >> sys.stderr, "[%s] Done processing events %s" % (spanki_utils.timestamp(), output_dir)
	
	summarize_events(eventdict,asta_summary_out)

	# A sample entry
	#{'attribute': 'transcript_id "FBtr0089158,FBtr0089162"; gene_id "chr4:89956-129430W"; flanks "null,92947-"; structure "1(2^,3(4^"; splice_chain "89956(90020^,91032(91193^";
	#sources "Undefined,Undefined"; dimension "2_6"; degree "4"; localization "5UTR_maxCDS"; \n', 'joins2': ['chr4:91194_92946:+'], 'joins1': ['chr4:90021_92946:+'], 
	#'sources': 'Undefined,Undefined', 'splice_chain': '89956(90020^,91032(91193^', 'seqname': 'chr4', 'end': '92947', 'eventcode': 'altFE', 'source': 'Undefined', 'unknown': '.',
	#'feature': 'as_event', 'start': '89956', 'score': '1.22222', 'strand': '+', 'gnames': 'FBgn0085432', 'degree': '4', 'localization': '5UTR_maxCDS', 'gene_id': 'chr4:89956-129430W', 
	#'transcript_id': 'FBtr0089158,FBtr0089162', 'flanks': 'null,92947-', 'structure': '1(2^,3(4^', 'gids': 'pan', 'txlist2': ['FBtr0089162'], 'txlist1': ['FBtr0089158', 'FBtr0089162'], 
	#'joinstring': 'chr4:90021_92946:+;chr4:91194_92946:+', 'dimension': '2_6'}


	# Checking for redundancy
	mergedict = collections.defaultdict(lambda : collections.defaultdict(dict))	
	events_by_joins = []
	duped_events = []	
	print "Checking for redundancy"
	probs = 0
	for eventid in eventdict.keys():
		'''
		Re-key on either joins or starts, to remove duplicates
		'''
		if (eventdict[eventid]['eventcode'] == "AltFE"):
			
			chainstring = eventdict[eventid]['splice_chain']
			#print chainstring
			chains = chainstring.split(",");  # List of sites
			pattern = re.compile('\d+\(')
			m = pattern.search(chains[0])
			try:
				start1 = m.group(0)
			except:
				start1 = "*"
				#print eventdict[eventid]
				probs += 1
			m = pattern.search(chains[1])
			try:
				start2 = m.group(0)
			except:
				start2 = "*"
				probs += 1
				#print eventdict[eventid]
			newid = str(start1) + str(start2)
		else:
			joins1 = eventdict[eventid]['joins1']
			joins2 = eventdict[eventid]['joins2']
			newid = str(joins1) + str(joins2)
		
		if newid in mergedict.keys():
			#print "Found a duplicate", eventdict[eventid]['eventcode'], newid
			mergedict[newid]['txlist1'].extend(eventdict[eventid]['txlist1'])
			mergedict[newid]['txlist2'].extend(eventdict[eventid]['txlist2'])
			mergedict[newid]['joins1'].extend(eventdict[eventid]['joins1'])
			mergedict[newid]['joins2'].extend(eventdict[eventid]['joins2'])
		else:
			mergedict[newid] = eventdict[eventid]

	# Now re-making event table
	eventdict = collections.defaultdict(lambda : collections.defaultdict(dict))	
	counter = 0
	excluded = 0
	for newid in mergedict.keys():
		counter += 1		
		eventid = 'ASTA' + "%07d" % (counter,)
		eventdict[eventid] = mergedict[newid]
		eventdict[eventid]['txlist1'] = uniq(eventdict[eventid]['txlist1'])
		eventdict[eventid]['txlist2'] = uniq(eventdict[eventid]['txlist2'])
		eventdict[eventid]['joins1'] = uniq(eventdict[eventid]['joins1'])
		eventdict[eventid]['joins2'] = uniq(eventdict[eventid]['joins2'])
	print "Processed", counter, "events"
	print "\tProblems:", probs
	
	"""
	Intersect with coverage data
	"""
	# Example ASTA line:
	#chrXHet	Undefined	as_event	69231	75522	1.20000	-	.	transcript_id "FBtr0113724,FBtr0113717/FBtr0113723/FBtr0302542"; gene_id "chrXHet:63778-75553C"; flanks "null,69231-"; structure "1(2^,3(4^"; splice_chain "75522(75431^,71449(71033^"; sources "Undefined,Undefined"; dimension "2_5"; degree "4"; localization "CDS_maxCDS"; 
	
	# Get a value for inclusion and for exclusion
	# The pointers to what to quantify are in 'joins1' and 'joins2' (For inclusion / exclusion respectively)
	# IF these are non-empty sets, incl/excl are just the coverages from these joins
	# If either is "none", then junctions can not be used to assay the structure
	# If one is is "irt", then the coverage is irt for ALL joins in alternative path
	# Calculate the inc,exc, for the event
	# also calculte inc,exc, for most 5' join in each path (call these inc_strict, excl_strict)
	#
	# Note than join strings are always sorted 5' to 3', so you can just get coverage of the first one only for 'strict'
	#
	
	get_coverage(eventdict,DATAB)
	# This gets the appropriate coverage,FPKM data for each defined event
	
		
	print >> sys.stderr, "[%s] Done iterating through events %s" % (spanki_utils.timestamp(), output_dir)

 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 	# Print final results table

 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 	# This is the one that is reported by default.  
 	# The other tables are only reported if verbose=T
 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

	'''
	sensitive mode table
	'''
	if cfile:
		fieldorder = ['event_id','gene_id','gene_name', 'eventcode','structure','transcript_id','joinstring','confounding_inccov','confounding_exccov', 'opt_calc_type','inccov','exccov','inc_sites','exc_sites','inc','exc', 'totalcov','avgcovpersite','psi','fpkm1','fpkm2','fpkm_proportion']
	else:
		fieldorder = ['event_id','gene_id','gene_name', 'eventcode','structure','transcript_id','joinstring','confounding_inccov','confounding_exccov', 'opt_calc_type','inccov','exccov','inc_sites','exc_sites','inc','exc', 'totalcov','avgcovpersite','psi']

	
	RESULTS = eventdict
	for i in RESULTS:
		RESULTS[i]['joinstring'] = RESULTS[i]['joinstring_sens']
		RESULTS[i]['inccov'] = RESULTS[i]['inccov_sens']
		RESULTS[i]['exccov'] = RESULTS[i]['exccov_sens'] 
		RESULTS[i]['inc_sites'] = RESULTS[i]['inc_sites_sens']
		RESULTS[i]['exc_sites'] = RESULTS[i]['exc_sites_sens'] 
		#print i, RESULTS[i]['incov'], RESULTS[i]['inccov_sens'], RESULTS[i]['excov'], RESULTS[i]['exccov_sens']
		#print "Before:", RESULTS[i]['opt_calc_type'], RESULTS[i]['inc'], RESULTS[i]['exc'], RESULTS[i]['inc_sens'], RESULTS[i]['exc_sens']
		RESULTS[i]['inc'] = RESULTS[i]['inc_sens']
		RESULTS[i]['exc'] = RESULTS[i]['exc_sens'] 
		#print "After:", RESULTS[i]['opt_calc_type'], RESULTS[i]['inc'], RESULTS[i]['exc'], RESULTS[i]['inc_sens'], RESULTS[i]['exc_sens']
		RESULTS[i]['totalcov'] = RESULTS[i]['totalcov_sens']
		RESULTS[i]['avgcovpersite'] = RESULTS[i]['avgcovpersite_sens'] 
		RESULTS[i]['psi'] = RESULTS[i]['psi_sens'] 

	print_output_table_firstiskey(RESULTS,asta_counts_out_sens,fieldorder)


	'''
	ALL modes (optimum, sensitive, precise)
	'''
	
	if (verbose != "F"):
		# print a precise mode table
		if cfile:
			fieldorder = ['event_id','gene_id','gene_name', 'eventcode','structure','transcript_id','joinstring_sens','inccov_sens','exccov_sens','totalcov_sens','avgcovpersite_sens','inc_sites_sens','exc_sites_sens','inc_sens','exc_sens','psi_sens','confounding_inccov','confounding_exccov', 'joinstring_prec','inccov_prec','exccov_prec','totalcov_prec','avgcovpersite_prec','inc_sites_prec','exc_sites_prec','inc_prec','exc_prec', 'psi','joinstring','inccov','exccov','totalcov','avgcovpersite','inc_sites','exc_sites','inc','exc', 'psi','fpkm1','fpkm2','fpkm_proportion']
		else:
			fieldorder = ['event_id','gene_id','gene_name', 'eventcode','structure','transcript_id','joinstring_sens','inccov_sens','exccov_sens','totalcov_sens','avgcovpersite_sens','inc_sites_sens','exc_sites_sens','inc_sens','exc_sens','psi_sens','confounding_inccov','confounding_exccov', 'joinstring_prec','inccov_prec','exccov_prec','totalcov_prec','avgcovpersite_prec','inc_sites_prec','exc_sites_prec','inc_prec','exc_prec', 'psi','joinstring','inccov','exccov','totalcov','avgcovpersite','inc_sites','exc_sites','inc','exc', 'psi']

		RESULTS = eventdict
		# This is deprecated
		#print_output_table_firstiskey(RESULTS,asta_counts_out_all,fieldorder)

		'''
		prec mode
		'''
		if cfile:
			fieldorder = ['event_id','gene_id','gene_name', 'eventcode','structure','transcript_id','joinstring','confounding_inccov','confounding_exccov', 'opt_calc_type','inccov','exccov','inc_sites','exc_sites','inc','exc', 'totalcov','avgcovpersite','psi','fpkm1','fpkm2','fpkm_proportion']
		else:
			fieldorder = ['event_id','gene_id','gene_name', 'eventcode','structure','transcript_id','joinstring','confounding_inccov','confounding_exccov', 'opt_calc_type','inccov','exccov','inc_sites','exc_sites','inc','exc', 'totalcov','avgcovpersite','psi']
	
		
		RESULTS = eventdict
		for i in RESULTS:
			RESULTS[i]['joinstring'] = RESULTS[i]['joinstring_prec']
			RESULTS[i]['inccov'] = RESULTS[i]['inccov_prec']
			RESULTS[i]['exccov'] = RESULTS[i]['exccov_prec'] 
			RESULTS[i]['inc_sites'] = RESULTS[i]['inc_sites_prec']
			RESULTS[i]['exc_sites'] = RESULTS[i]['exc_sites_prec'] 
			RESULTS[i]['inc'] = RESULTS[i]['inc_prec']
			RESULTS[i]['exc'] = RESULTS[i]['exc_prec'] 
			RESULTS[i]['totalcov'] = RESULTS[i]['totalcov_prec']
			RESULTS[i]['avgcovpersite'] = RESULTS[i]['avgcovpersite_prec'] 
			RESULTS[i]['psi'] = RESULTS[i]['psi_prec'] 
		print_output_table_firstiskey(RESULTS,asta_counts_out_prec,fieldorder)
	
		'''
		opt mode
		-- Deprecated -- 
		'''
		# This was a convenience table, and it is leading to confusion, removed for now
		#if cfile:
		#	fieldorder = ['event_id','gene_id','gene_name', 'eventcode','structure','transcript_id','joinstring_sens','confounding_inccov','confounding_exccov', 'opt_calc_type','joinstring','inccov','exccov','inc_sites','exc_sites','inc','exc', 'totalcov','avgcovpersite','psi','fpkm1','fpkm2','fpkm_proportion']
		#else:
		#	fieldorder = ['event_id','gene_id','gene_name', 'eventcode','structure','transcript_id','joinstring_sens','confounding_inccov','confounding_exccov', 'opt_calc_type','joinstring','inccov','exccov','totalcov','avgcovpersite','inc_sites','exc_sites','inc','exc', 'psi']
	
		
		#RESULTS = eventdict
		# This is deprecated
		#print_output_table_firstiskey(RESULTS,asta_counts_out_opt,fieldorder)


	'''
	Print the unassayable events
	'''

	fieldorder = ['event_id','gene_id','gene_name','eventcode','structure','transcript_id',
	'joinstring']
	RESULTS = unassayable_events
	print_output_table_firstiskey(RESULTS,unassayable_events_out,fieldorder)
			
if __name__ == "__main__":
    sys.exit(main())





